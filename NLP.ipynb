{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52701ac0-d967-42b5-bd08-a385a5d04899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in d:\\python\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\python\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.5 MB 960.0 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d1a1d9c-6cc2-44cf-94d7-a9db9cb9b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0e9f99-105e-4fcd-bd14-481ab99645d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"My name is Sarthak. This is my first time learning NLP!\n",
    "This is an example corpus.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d041c1-f2cd-4cde-a950-028a427bfa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Sarthak. This is my first time learning NLP!\\nThis is an example corpus.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0b540ec-037c-4e52-8220-173913c62e22",
   "metadata": {},
   "source": [
    "--->Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677db620-563c-4ade-9e12-2a4593cdbe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paragraph-->sentences\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a72eab-e467-466d-b612-14b7ef7b76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarthak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda4ea89-6511-4ea8-91d4-3e00ae822de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b0a46da-5b64-439d-bf04-d3f203a8f02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sarthak',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'time',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'corpus',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentences-->words\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5d78ed6-9b35-4c15-917d-43719a43b8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Sarthak', '.']\n",
      "['This', 'is', 'my', 'first', 'time', 'learning', 'NLP', '!']\n",
      "['This', 'is', 'an', 'example', 'corpus', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d9b8d24-9be0-41c2-9266-699f8ba95209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0fd74c6-ddc5-43a5-a9f4-310cdf3790c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sarthak',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'time',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'corpus',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8705f172-45de-4de8-96a9-fa16aa7fc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58f684b5-d9da-41e7-bf56-867208f39c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26267b01-f549-4cb1-b17c-78e782597d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sarthak.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'time',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'corpus',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "656efe55-96dc-40eb-a606-56a94ec59901",
   "metadata": {},
   "source": [
    "--->Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e491db9-fada-494c-9eba-39711199da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eat\", \"ate\", \"eating\", \"eaten\", \"eats\", \"history\", \"historical\", \"historically\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be9b7195-ede7-4a63-8c01-02e2b9e82cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PorterStemmer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2e8fc56-7813-43b3-b258-f3a9a258278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fba86e6f-f2f2-49f9-8ee7-9b7864409357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat-->eat\n",
      "ate-->ate\n",
      "eating-->eat\n",
      "eaten-->eaten\n",
      "eats-->eat\n",
      "history-->histori\n",
      "historical-->histor\n",
      "historically-->histor\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"-->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d9e9a03-3edb-4706-8ddf-04cf333b832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegExpStemmer\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "796ee416-707d-478e-92d7-97ebfa329939",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c88c86ad-720b-4996-969d-11b9d2686088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd2d7aff-2e64-4bbc-b6c7-2a6e45500540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SnowballStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0fd47ad-a11f-4d12-ad5d-42a79ef58584",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60c9def3-3d89-48cb-ae2c-601802ddfe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat-->eat\n",
      "ate-->ate\n",
      "eating-->eat\n",
      "eaten-->eaten\n",
      "eats-->eat\n",
      "history-->histori\n",
      "historical-->histor\n",
      "historically-->histor\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'-->'+sb.stem(word))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f8c8675-5041-4d9b-86f7-8ead83ebe66f",
   "metadata": {},
   "source": [
    "--->Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "babebae2-cc10-4555-849d-70b9d2b7337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae3986a2-396a-439a-8b10-09c39524b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sarthak\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d2419e2-0081-4426-b7b9-fef7dd6c85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6eba33e-f3ce-42cf-ac22-3ae02d8d4a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat-->eat\n",
      "ate-->eat\n",
      "eating-->eat\n",
      "eaten-->eat\n",
      "eats-->eat\n",
      "history-->history\n",
      "historical-->historical\n",
      "historically-->historically\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"-->\"+lem.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba13852b-71af-4c0d-84a9-a4faa3ca464c",
   "metadata": {},
   "source": [
    "--->Text Preprocessing (Stopwords and Parts of Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69955733-6b5c-4cd1-afec-41ea987ffee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac54e85f-98fa-4a77-b4c2-1e3652b69a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sarthak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "747b4fff-db9a-4c45-9832-d97725ebb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "503560af-312c-4e39-9d09-c623548f4c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')\n",
    "#gives you a very long list of stopwords in english language\n",
    "#can also try for other available languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79972f4f-9d9f-470d-8f5b-85c0452f5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(paragraph)\n",
    "#we get a list of all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c356cb59-e394-4cd0-b4b0-097485982520",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c680f227-feb8-43d2-829f-55e72d4a0c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00677361-56e9-430b-a224-261dc91fb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sb=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a75f33d6-3b17-42c0-892d-26a5112aba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[sb.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc658644-3891-49b0-bc44-052285633e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'on strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76e3673f-76fa-42ef-8587-e313c3eb9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9da99236-ee3a-4387-ad3b-305d8d73cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[lem.lemmatize(word.lower(), pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65c37340-7b69-41ca-b4d1-de56df1f979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , come loot us , take .',\n",
       " 'yet do nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india get first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28654a8d-1474-4305-8090-4d6a40db65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ccd56282-f9cb-4d92-b32d-63b555be3737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sarthak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee363bf5-9e74-4945-87c5-3acb047cae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
      "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('Why', 'WRB'), ('?', '.')]\n",
      "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
      "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
      "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
      "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
      "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
      "[('I', 'PRP'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
      "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
      "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('Dept', 'NNP'), ('.', '.')]\n",
      "[('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeeded', 'VBD'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#tagging parts of speech for each word in the paragraph\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3dd0e4d4-0139-4154-875d-30f70cf76859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition\n",
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de322268-eeb2-492e-94bb-7325db8dd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c614af1-d057-4fe3-8fab-48cd6fa9165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3eb1840c-7b62-4126-80d5-d34842a4cda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sarthak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sarthak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "072d8d41-3c46-4753-adf4-49cbc85ee944",
   "metadata": {},
   "source": [
    "nltk.ne_chunk(tag_elements).draw()\n",
    "\n",
    "The above command gives you a tree diagram of the named entities in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefadf3c-8759-41be-a49f-bb241c1c1a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3cc03b4a-1696-45fa-8c09-d8f250273455",
   "metadata": {},
   "source": [
    "Average Word2Vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb952ff4-c82a-4f57-b590-8f9514b3c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\python\\lib\\site-packages (24.1)\n",
      "Requirement already satisfied: setuptools in d:\\python\\lib\\site-packages (70.1.1)\n",
      "Requirement already satisfied: wheel in d:\\python\\lib\\site-packages (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0d5bb1-36f4-48b2-8f5e-88718dc8119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.2.tar.gz (23.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\python\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\python\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Building wheels for collected packages: gensim\n",
      "  Building wheel for gensim (pyproject.toml): started\n",
      "  Building wheel for gensim (pyproject.toml): finished with status 'error'\n",
      "Failed to build gensim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for gensim (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [714 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-312\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\downloader.py -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\interfaces.py -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\matutils.py -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\nosy.py -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\utils.py -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\bleicorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\csvcorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\dictionary.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\hashdictionary.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\indexedcorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\lowcorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\malletcorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\mmcorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\opinosiscorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\sharded_corpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\svmlightcorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\textcorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\ucicorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\wikicorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\corpora\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\atmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\basemodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\bm25model.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\callbacks.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\coherencemodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\doc2vec.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\ensemblelda.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\fasttext.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\hdpmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\keyedvectors.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\ldamodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\ldamulticore.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\ldaseqmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\lda_dispatcher.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\lda_worker.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\logentropy_model.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\lsimodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\lsi_dispatcher.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\lsi_worker.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\nmf.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\normmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\phrases.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\poincare.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\rpmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\tfidfmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\translation_matrix.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\word2vec.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\_fasttext_bin.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\parsing\n",
      "  copying gensim\\parsing\\porter.py -> build\\lib.win-amd64-cpython-312\\gensim\\parsing\n",
      "  copying gensim\\parsing\\preprocessing.py -> build\\lib.win-amd64-cpython-312\\gensim\\parsing\n",
      "  copying gensim\\parsing\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\\parsing\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\benchmark.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\glove2word2vec.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\make_wiki.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\make_wikicorpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\make_wiki_online.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\make_wiki_online_nodebug.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\package_info.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\segment_wiki.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\word2vec2tensor.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\word2vec_standalone.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  copying gensim\\scripts\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\\scripts\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  copying gensim\\similarities\\annoy.py -> build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  copying gensim\\similarities\\docsim.py -> build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  copying gensim\\similarities\\levenshtein.py -> build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  copying gensim\\similarities\\nmslib.py -> build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  copying gensim\\similarities\\termsim.py -> build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  copying gensim\\similarities\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\basetmtests.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\simspeed.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\simspeed2.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\svd_error.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_aggregation.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_api.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_atmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_big.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_bm25model.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_coherencemodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_corpora.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_corpora_dictionary.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_corpora_hashdictionary.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_datatype.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_direct_confirmation.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_doc2vec.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_ensemblelda.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_fasttext.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_glove2word2vec.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_hdpmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_indirect_confirmation.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_keyedvectors.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_ldamodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_ldaseqmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_lda_callback.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_lee.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_logentropy_model.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_lsimodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_matutils.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_miislita.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_nmf.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_normmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_parsing.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_phrases.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_poincare.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_probability_estimation.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_rpmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_scripts.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_segmentation.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_sharded_corpus.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_similarities.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_similarity_metrics.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_text_analysis.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_tfidfmodel.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_tmdiff.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_translation_matrix.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_utils.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\test_word2vec.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\utils.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  copying gensim\\test\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\\test\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  copying gensim\\topic_coherence\\aggregation.py -> build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  copying gensim\\topic_coherence\\direct_confirmation_measure.py -> build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  copying gensim\\topic_coherence\\indirect_confirmation_measure.py -> build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  copying gensim\\topic_coherence\\probability_estimation.py -> build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  copying gensim\\topic_coherence\\segmentation.py -> build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  copying gensim\\topic_coherence\\text_analysis.py -> build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  copying gensim\\topic_coherence\\__init__.py -> build\\lib.win-amd64-cpython-312\\gensim\\topic_coherence\n",
      "  running egg_info\n",
      "  writing gensim.egg-info\\PKG-INFO\n",
      "  writing dependency_links to gensim.egg-info\\dependency_links.txt\n",
      "  writing requirements to gensim.egg-info\\requires.txt\n",
      "  writing top-level names to gensim.egg-info\\top_level.txt\n",
      "  reading manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'COPYING'\n",
      "  writing manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.corpora' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.corpora' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.corpora' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.corpora' to be distributed and are\n",
      "          already explicitly excluding 'gensim.corpora' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.models' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.models' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.models' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.models' to be distributed and are\n",
      "          already explicitly excluding 'gensim.models' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.similarities' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.similarities' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.similarities' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.similarities' to be distributed and are\n",
      "          already explicitly excluding 'gensim.similarities' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.test.test_data' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.test.test_data' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.test.test_data' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.test.test_data' to be distributed and are\n",
      "          already explicitly excluding 'gensim.test.test_data' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.test.test_data.DTM' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.test.test_data.DTM' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.test.test_data.DTM' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.test.test_data.DTM' to be distributed and are\n",
      "          already explicitly excluding 'gensim.test.test_data.DTM' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.test.test_data.PathLineSentences' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.test.test_data.PathLineSentences' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.test.test_data.PathLineSentences' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.test.test_data.PathLineSentences' to be distributed and are\n",
      "          already explicitly excluding 'gensim.test.test_data.PathLineSentences' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.test.test_data.old_d2v_models' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.test.test_data.old_d2v_models' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.test.test_data.old_d2v_models' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.test.test_data.old_d2v_models' to be distributed and are\n",
      "          already explicitly excluding 'gensim.test.test_data.old_d2v_models' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  C:\\Users\\sarthak\\AppData\\Local\\Temp\\pip-build-env-2bq1rrnl\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:215: _Warning: Package 'gensim.test.test_data.old_w2v_models' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'gensim.test.test_data.old_w2v_models' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'gensim.test.test_data.old_w2v_models' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'gensim.test.test_data.old_w2v_models' to be distributed and are\n",
      "          already explicitly excluding 'gensim.test.test_data.old_w2v_models' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying gensim\\_matutils.c -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\_matutils.pyx -> build\\lib.win-amd64-cpython-312\\gensim\n",
      "  copying gensim\\corpora\\_mmreader.c -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\models\\doc2vec_corpusfile.cpp -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\doc2vec_inner.cpp -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\fasttext_corpusfile.cpp -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\fasttext_inner.c -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\nmf_pgd.c -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\word2vec_corpusfile.cpp -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\word2vec_inner.c -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\similarities\\fastss.c -> build\\lib.win-amd64-cpython-312\\gensim\\similarities\n",
      "  copying gensim\\corpora\\_mmreader.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\corpora\n",
      "  copying gensim\\models\\doc2vec_corpusfile.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\doc2vec_inner.pxd -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\doc2vec_inner.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\fast_line_sentence.h -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\fasttext_corpusfile.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\fasttext_inner.pxd -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\fasttext_inner.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\nmf_pgd.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\stdint_wrapper.h -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\voidptr.h -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\word2vec_corpusfile.pxd -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\word2vec_corpusfile.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\word2vec_inner.pxd -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  copying gensim\\models\\word2vec_inner.pyx -> build\\lib.win-amd64-cpython-312\\gensim\\models\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\OPUS_en_it_europarl_train_one2ten.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\alldata-id-10.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\atmodel_3_0_1_model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\atmodel_3_0_1_model.expElogbeta.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\atmodel_3_0_1_model.id2word -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\atmodel_3_0_1_model.state -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\bgwiki-latest-pages-articles-shortened.xml.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\compatible-hash-true.model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\cp852_fasttext.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\crime-and-punishment.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\crime-and-punishment.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\crime-and-punishment.vec -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\d2v-lee-v0.13.0 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\doc2vec_old -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\doc2vec_old_sep -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\doc2vec_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\doc2vec_old_sep.syn1neg.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\dtm_test.dict -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\dtm_test.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ensemblelda -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\enwiki-table-markup.xml.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\euclidean_vectors.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\fasttext_old -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\fasttext_old_sep -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\fasttext_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\fasttext_old_sep.syn1neg.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\fb-ngrams.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ft_kv_3.6.0.model.gz -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ft_model_2.3.0 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\head500.noblanks.cor -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\head500.noblanks.cor.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\head500.noblanks.cor_tfidf.model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\head500.noblanks.cor_wordids.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\high_precision.kv.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\high_precision.kv.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\large_tag_doc_10_iter50 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lda_3_0_1_model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lda_3_0_1_model.expElogbeta.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lda_3_0_1_model.id2word -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lda_3_0_1_model.state -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_2_7 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_2_7.expElogbeta.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_2_7.id2word -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_2_7.state -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_3_5 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_3_5.expElogbeta.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_3_5.id2word -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldamodel_python_3_5.state -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldavowpalwabbit.dict.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\ldavowpalwabbit.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lee.cor -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lee_background.cor -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lee_fasttext -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lee_fasttext.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lee_fasttext.vec -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\lee_fasttext_new.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\miIslita.cor -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\mini_newsgroup -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\model-from-gensim-3.8.0.w2v -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\nmf_model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\non_ascii_fasttext.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\old_keyedvectors_320.dat -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\pang_lee_polarity.cor -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\pang_lee_polarity_fasttext.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\pang_lee_polarity_fasttext.vec -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\para2para_text1.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\para2para_text2.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phraser-3.6.0.model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phraser-no-common-terms.pkl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phraser-no-scoring.pkl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phraser-scoring-str.pkl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phrases-3.6.0.model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phrases-no-common-terms.pkl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phrases-no-scoring.pkl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\phrases-scoring-str.pkl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\poincare_cp852.tsv -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\poincare_hypernyms.tsv -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\poincare_hypernyms_large.tsv -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\poincare_test_3.4.0 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\poincare_utf8.tsv -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\poincare_vectors.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\pre_0_13_2_model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\pre_0_13_2_model.state -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\pretrained.vec -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\questions-words.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\reproduce.dat -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\reproduce.dat.gz -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\similarities0-1.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\simlex999.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\small_tag_doc_5_iter50 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_corpus_ok.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_corpus_small.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_glove.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_mmcorpus_corrupt.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm.gz -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_mmcorpus_overflow.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_mmcorpus_with_index.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\test_mmcorpus_with_index.mm.index -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.blei -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.blei.index -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.blei.vocab -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.low -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.low.index -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.mallet -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.mallet.index -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.mm -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.mm.index -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.svmlight -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.svmlight.index -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.uci -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.uci.index -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.uci.vocab -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\testcorpus.xml.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\tfidf_model.tst -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\tfidf_model.tst.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\tfidf_model_3_2.tst -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\toy-data.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\toy-model-pretrained.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\toy-model.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\toy-model.vec -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\varembed_lee_subcorpus.cor -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\varembed_morfessor.bin -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\varembed_vectors.pkl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\w2v-lee-v0.12.0 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\w2v_keyedvectors_load_test.modeldata -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\w2v_keyedvectors_load_test.vocab -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_3.3 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_old -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_old_sep -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_old_sep.syn1neg.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_c -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_py2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_py3 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_py3_4 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.neg_labels.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn0.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn0_lockf.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn1neg.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.neg_labels.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn0.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn0_lockf.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn1neg.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.neg_labels.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn0.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn0_lockf.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn1neg.npy -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  copying gensim\\test\\test_data\\wordsim353.tsv -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\DTM\n",
      "  copying gensim\\test\\test_data\\DTM\\ldaseq_3_0_1_model -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\DTM\n",
      "  copying gensim\\test\\test_data\\DTM\\sstats_test.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\DTM\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\PathLineSentences\n",
      "  copying gensim\\test\\test_data\\PathLineSentences\\1.txt -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\PathLineSentences\n",
      "  copying gensim\\test\\test_data\\PathLineSentences\\2.txt.bz2 -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\PathLineSentences\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.1.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.2.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.3.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.4.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.1.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.2.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.3.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.4.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_1.0.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_1.0.1.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.0.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.1.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.2.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.3.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.0.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.1.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.2.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.3.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.4.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_d2v_models\n",
      "  creating build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.1.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.2.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.3.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.4.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.1.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.2.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.3.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.4.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_1.0.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_1.0.1.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.0.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.1.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.2.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.3.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.0.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.1.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.2.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.3.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.4.0.mdl -> build\\lib.win-amd64-cpython-312\\gensim\\test\\test_data\\old_w2v_models\n",
      "  running build_ext\n",
      "  building 'gensim.models.word2vec_inner' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for gensim\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (gensim)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f478ee7a-046d-42a3-9e9f-2dd743dbdd93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b24d12-0008-4e88-ae93-7e2cfe7f4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVector\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d1c60-5891-48d3-a432-fa5aa4f2f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')\n",
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889648b-1ba5-4661-bf53-4a16d9a16853",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b37f2ea-162c-43b2-b66c-7079b51b2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=wv['king']-wv['man']+wv['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893f0c9-f5bb-4bec-90eb-e25ae3b56a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar([vec])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc1d6eda-a612-4081-867a-b052b19bbf3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e9327644-9605-41b2-ab25-ae8947fcf2ea",
   "metadata": {},
   "source": [
    "Moodule gensim wasn't working so the same code has been implemented in spaCy below.\n",
    "spaCy is a module of python for advanced NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c9f9386-d01e-445c-a20f-f32f7007d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\python\\lib\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\python\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\python\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\python\\lib\\site-packages (from spacy) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in d:\\python\\lib\\site-packages (from spacy) (70.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\python\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in d:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\python\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\python\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\python\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.7.5-cp312-cp312-win_amd64.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.7 MB 3.3 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.3/11.7 MB 4.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.6/11.7 MB 5.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.1/11.7 MB 7.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.9/11.7 MB 10.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.8/11.7 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.6/11.7 MB 13.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.4/11.7 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.9/11.7 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.4/11.7 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/11.7 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/11.7 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/11.7 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.8/11.7 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.7 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.6/11.7 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.0/11.7 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.2/11.7 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.5/11.7 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.7/11.7 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.3/11.7 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.0/11.7 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.9/11.7 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.7 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.4/11.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.7/11.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.7/11.7 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB ? eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.4/122.4 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "   ---------------------------------------- 0.0/478.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 478.8/478.8 kB 15.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.0/1.4 MB 20.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 18.4 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB ? eta 0:00:00\n",
      "Downloading blis-0.7.11-cp312-cp312-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.9/6.6 MB 28.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.9/6.6 MB 23.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 23.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 21.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.3/6.6 MB 19.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 18.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.2/6.6 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.4 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.9/5.4 MB 19.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.7/5.4 MB 21.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.5/5.4 MB 20.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.0/5.4 MB 19.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 18.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.4 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 15.7 MB/s eta 0:00:00\n",
      "Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.0-cp312-cp312-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.1/151.1 kB 8.8 MB/s eta 0:00:00\n",
      "Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34699695-2e74-4705-86d3-34e666aaf065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/42.8 MB 1.7 MB/s eta 0:00:26\n",
      "     ---------------------------------------- 0.2/42.8 MB 2.4 MB/s eta 0:00:19\n",
      "     ---------------------------------------- 0.5/42.8 MB 4.2 MB/s eta 0:00:11\n",
      "      --------------------------------------- 1.0/42.8 MB 6.5 MB/s eta 0:00:07\n",
      "     - -------------------------------------- 1.8/42.8 MB 9.8 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.8/42.8 MB 11.2 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 3.8/42.8 MB 12.8 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 4.6/42.8 MB 13.5 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 5.3/42.8 MB 13.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 6.0/42.8 MB 13.7 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 6.4/42.8 MB 13.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 6.4/42.8 MB 13.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 6.4/42.8 MB 13.6 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 7.3/42.8 MB 11.6 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 7.7/42.8 MB 11.8 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 8.7/42.8 MB 12.4 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 9.4/42.8 MB 12.5 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 9.9/42.8 MB 12.4 MB/s eta 0:00:03\n",
      "     --------- ----------------------------- 10.4/42.8 MB 13.6 MB/s eta 0:00:03\n",
      "     --------- ----------------------------- 11.0/42.8 MB 13.9 MB/s eta 0:00:03\n",
      "     ---------- ---------------------------- 11.7/42.8 MB 13.4 MB/s eta 0:00:03\n",
      "     ----------- --------------------------- 12.2/42.8 MB 13.4 MB/s eta 0:00:03\n",
      "     ----------- --------------------------- 12.8/42.8 MB 13.1 MB/s eta 0:00:03\n",
      "     ------------ -------------------------- 13.4/42.8 MB 12.8 MB/s eta 0:00:03\n",
      "     ------------ -------------------------- 14.0/42.8 MB 12.6 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 14.7/42.8 MB 12.4 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 15.3/42.8 MB 12.4 MB/s eta 0:00:03\n",
      "     -------------- ------------------------ 15.9/42.8 MB 12.4 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 16.5/42.8 MB 12.1 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 17.2/42.8 MB 13.9 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 17.7/42.8 MB 13.6 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 18.2/42.8 MB 13.9 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 19.0/42.8 MB 13.4 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 19.5/42.8 MB 13.4 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 20.1/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 20.7/42.8 MB 13.4 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 21.3/42.8 MB 13.4 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 21.9/42.8 MB 13.4 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 22.4/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 22.9/42.8 MB 12.8 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 23.5/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 24.1/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 24.6/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 25.1/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 25.6/42.8 MB 12.8 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 26.2/42.8 MB 13.4 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 26.9/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 27.5/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 28.1/42.8 MB 12.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 28.5/42.8 MB 13.4 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 29.1/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 29.7/42.8 MB 13.1 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 30.2/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 30.8/42.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 31.4/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 32.1/42.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 32.7/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 33.3/42.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 33.9/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.5/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 35.1/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 35.7/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 36.2/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 36.8/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 37.5/42.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 38.1/42.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.7/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 39.3/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.9/42.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 40.5/42.8 MB 12.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 41.2/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.8/42.8 MB 12.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.5/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.8/42.8 MB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in d:\\python\\lib\\site-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (70.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\python\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\python\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in d:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\python\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\python\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\python\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\python\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\python\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\python\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\python\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.17.2)\n",
      "Requirement already satisfied: wrapt in d:\\python\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\python\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54fb8f89-9d43-4683-8061-0a257333dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "180014f1-d86b-4425-9c9e-da5a413e7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57df301d-67cd-4bb6-9a90-746ea21c25ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1296e-01, -4.1865e+00, -1.8453e+00,  3.0781e-01,  2.4956e+00,\n",
       "        9.6267e-01, -1.8161e+00,  4.4655e+00, -2.8210e+00,  9.7090e-01,\n",
       "        1.3542e+01,  4.3195e-01, -5.3098e+00,  4.7098e+00,  2.9030e+00,\n",
       "        1.5588e+00,  6.0064e+00, -3.0345e+00,  1.0626e+00, -7.7197e-01,\n",
       "       -5.4771e+00, -9.7380e-01, -4.4345e+00,  5.8367e+00,  2.4302e+00,\n",
       "       -3.9408e+00, -9.1862e-01, -4.9124e+00,  1.4591e+00, -7.2772e-01,\n",
       "        3.4957e+00, -4.0077e+00, -1.8354e+00, -4.1052e+00,  4.9211e+00,\n",
       "       -9.7053e-01,  1.9223e+00,  5.2605e+00,  1.6086e+00,  7.1328e-01,\n",
       "       -1.2146e+00, -1.9869e+00,  8.0265e-01,  2.9298e+00,  7.2985e-01,\n",
       "       -6.2892e-01, -1.7082e+00,  1.9893e+00,  4.7529e-01,  3.2264e+00,\n",
       "       -3.9215e+00,  4.6556e+00,  1.3475e+00, -1.0979e+00, -3.0365e+00,\n",
       "        1.5815e+00,  2.2835e+00, -4.0616e+00,  2.5730e+00,  4.0618e+00,\n",
       "        9.5438e-01, -6.2563e+00,  5.6463e+00, -3.8933e+00,  4.4076e+00,\n",
       "        2.0517e+00, -6.6906e+00, -6.9448e+00,  6.0371e+00,  9.3081e-01,\n",
       "        1.5180e+00,  2.3974e+00, -3.8043e+00, -4.3941e+00, -3.6979e+00,\n",
       "        2.9489e+00, -8.9735e+00,  9.5273e+00, -6.4149e-01,  2.2565e+00,\n",
       "       -7.2062e+00, -1.0078e+00, -4.4381e+00,  2.0424e+00, -6.6736e-01,\n",
       "        4.3500e+00, -1.6199e+00,  3.1975e+00, -1.2065e+00, -6.5684e-01,\n",
       "        7.5759e-01, -1.6033e+00,  2.5450e+00, -5.4999e+00, -1.8909e+00,\n",
       "       -1.2985e-02,  2.6703e+00,  5.4623e-01, -2.4504e+00, -4.4326e-01,\n",
       "       -1.7250e+00,  9.1585e-01,  7.5243e+00, -5.8451e-01,  3.4550e+00,\n",
       "        3.4817e+00, -4.1599e+00, -5.5125e-01,  2.7681e-02, -3.1687e+00,\n",
       "       -4.8459e+00,  7.9108e+00, -1.7062e+00, -2.6731e+00,  9.7841e+00,\n",
       "        3.8851e+00, -3.7930e+00, -5.2979e-01,  6.6191e-01, -9.7232e-01,\n",
       "       -9.4692e-01, -4.4918e+00,  1.0932e+00, -4.3751e+00,  1.3182e-02,\n",
       "       -1.0243e+01,  4.7973e+00, -8.7426e+00,  2.5479e+00,  2.3454e+00,\n",
       "       -6.4140e+00,  7.3875e-01,  5.8565e+00, -2.5964e-01,  1.6558e+00,\n",
       "       -3.1353e+00, -6.6752e+00,  1.0550e+00,  1.7017e+00, -3.8360e+00,\n",
       "       -1.1980e+01, -1.3750e+00, -1.9261e+00,  3.1267e+00,  3.2874e+00,\n",
       "       -2.8928e+00, -1.0893e+01,  4.2848e+00, -4.0890e-02, -5.9565e-01,\n",
       "       -3.3473e-02,  1.6832e+00,  2.1454e-01,  7.2849e+00,  2.8116e+00,\n",
       "        2.5708e+00, -3.9823e-01, -1.7257e+00, -6.1063e+00, -4.2618e+00,\n",
       "       -3.3886e+00, -9.2663e+00,  1.7600e-01, -3.3873e-02, -3.7070e+00,\n",
       "       -9.1995e+00, -7.1594e+00, -6.0189e-01, -7.2560e-01,  1.5342e+00,\n",
       "        5.1083e+00,  2.4373e+00, -3.8012e+00, -2.1752e-01,  2.9503e+00,\n",
       "       -2.5551e+00,  4.9827e-01,  8.6823e-01, -4.3449e+00, -4.3821e+00,\n",
       "        3.4993e+00, -1.9518e+00,  2.2036e+00, -6.6526e-01,  7.1015e+00,\n",
       "        3.6784e+00,  2.6251e-01,  1.5379e+00, -8.1950e-01,  1.1065e+00,\n",
       "        3.3167e+00, -5.9392e+00, -4.0191e+00,  2.6496e+00,  2.3168e+00,\n",
       "       -8.5681e-02, -3.5059e+00,  1.5915e+00, -3.1831e-01,  6.9366e+00,\n",
       "        3.8439e+00,  9.4076e-01, -7.5424e+00,  2.7847e+00, -2.2814e+00,\n",
       "       -4.2487e+00, -2.6604e-01,  3.7954e+00, -3.6526e+00,  4.3823e+00,\n",
       "       -2.6506e+00,  3.5298e+00,  2.2597e+00,  6.3055e+00, -7.0194e-01,\n",
       "        4.1565e+00,  8.2306e+00,  5.7675e-01,  4.3596e-01, -8.8400e+00,\n",
       "       -3.0249e+00,  4.0032e+00,  2.4232e+00,  6.9885e+00, -2.5906e-01,\n",
       "       -4.2059e+00,  1.2643e+00,  1.0110e+01,  9.7016e-01,  2.2963e+00,\n",
       "       -1.2802e+00, -1.4447e+00, -3.4386e+00,  5.6555e+00,  3.3911e+00,\n",
       "        6.9418e+00, -6.8705e+00, -8.1536e-01, -7.2334e+00,  3.0509e+00,\n",
       "        8.7676e-01,  6.4216e+00, -3.1655e+00, -1.5308e+00, -1.1056e+00,\n",
       "       -5.0426e+00,  4.6801e+00,  4.6812e+00,  4.0401e+00, -3.7289e-01,\n",
       "        6.7437e-01, -8.6660e+00, -9.9656e+00,  2.4979e+00, -1.4783e-01,\n",
       "       -5.6301e+00,  4.5542e+00,  4.8165e+00, -2.2055e-01,  4.5169e+00,\n",
       "        1.7496e+00,  2.9019e-01, -1.1683e+00, -4.3981e-01,  2.3469e+00,\n",
       "       -4.3521e-02,  6.3715e-01,  5.8259e-01, -8.5701e+00,  4.6419e+00,\n",
       "        2.3809e+00, -1.9273e-01, -6.9772e+00,  7.6172e-01, -6.3895e-01,\n",
       "       -3.3769e+00,  6.1265e+00, -1.9695e+00, -2.3404e+00,  6.6789e+00,\n",
       "       -3.5265e+00, -3.3883e+00,  6.1372e+00,  4.5550e+00,  6.0957e+00,\n",
       "       -2.2007e-01,  6.2087e-01,  2.5527e+00, -4.5590e+00, -2.8429e+00,\n",
       "        2.0645e+00, -1.6221e+00, -2.8171e+00, -2.9680e+00,  1.3651e+00,\n",
       "        3.6137e+00, -3.2096e-01, -1.9346e+00, -4.8738e+00,  2.5565e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"king\"\n",
    "vector = nlp.vocab[word].vector\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb814c19-1018-4bc9-9783-6732b03b1c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7471dae-43e0-4109-8098-6a707a6d68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#function to evaluate the dot product between two word vectors\n",
    "def dot(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "#function to return the most similar vectors by comparing dot products\n",
    "def most_similar(word, top_n=10):\n",
    "    word_vec = nlp.vocab[word].vector\n",
    "    all_words = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.is_alpha]\n",
    "    \n",
    "    similarities = [(w.text, dot(word_vec, w.vector)) for w in all_words]\n",
    "    similarities = sorted(similarities, key=lambda item: -item[1])\n",
    "    \n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cb35f96-66cb-4a4d-ad14-9d1bc7ae97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7c73973-d0ac-42b3-9032-f36a58ff90b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 1.0000001),\n",
       " ('that', 0.43361616),\n",
       " ('nothin', 0.40930462),\n",
       " ('havin', 0.40479523),\n",
       " ('somethin', 0.39718974),\n",
       " ('there', 0.37138095),\n",
       " ('where', 0.36384344),\n",
       " ('those', 0.36091805),\n",
       " ('they', 0.360211),\n",
       " ('and', 0.3598572)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "444c9e98-b39e-4c70-9085-e494d8ae66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=nlp.vocab['king'].vector-nlp.vocab['man'].vector+nlp.vocab['woman'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f147e33-16e9-4882-9825-5b571d6ae17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6178014\n"
     ]
    }
   ],
   "source": [
    "queen_vec=nlp.vocab['queen'].vector\n",
    "print(dot(vec, queen_vec))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e919045-05fe-42cc-9c63-7dd75d989089",
   "metadata": {},
   "source": [
    "Quite the similarity!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
